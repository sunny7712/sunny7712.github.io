<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Linear Regression From Scratch | Vamsi&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="In depth explanation of one of the most fundamental Machine Learning algorithm.">
<meta name="author" content="Vamsi">
<link rel="canonical" href="http://localhost:1313/docs/linear_regression/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/linear_regression/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
    <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"
  integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib"
  crossorigin="anonymous"
>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js"
  integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh"
  crossorigin="anonymous">
</script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js"
  integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
  crossorigin="anonymous"
  onload="renderMathInElement(document.body);">
</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '\\(', right: '\\)', display: false},  
        {left: "$", right: "$", display: false} 
      ],
      throwOnError : false
    });
  });
</script>


  <meta property="og:url" content="http://localhost:1313/docs/linear_regression/">
  <meta property="og:site_name" content="Vamsi&#39;s Blog">
  <meta property="og:title" content="Linear Regression From Scratch">
  <meta property="og:description" content="In depth explanation of one of the most fundamental Machine Learning algorithm.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2025-02-28T19:49:18+05:30">
    <meta property="article:modified_time" content="2025-02-28T19:49:18+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Linear Regression From Scratch">
<meta name="twitter:description" content="In depth explanation of one of the most fundamental Machine Learning algorithm.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Docs",
      "item": "http://localhost:1313/docs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Linear Regression From Scratch",
      "item": "http://localhost:1313/docs/linear_regression/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Linear Regression From Scratch",
  "name": "Linear Regression From Scratch",
  "description": "In depth explanation of one of the most fundamental Machine Learning algorithm.",
  "keywords": [
    
  ],
  "articleBody": "I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, we’ll go through a bit on how we tackle differentiation of vectors and matrices in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet’s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must’ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you’re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it’s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model’s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet’s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote: Here, we derived the loss function of Linear Regression intuitively. There’s also a more rigorous mathematical derivation which we will discuss later in the blog.\nSolution for the Equation If you can recall, to find the minimum of any function, we take the derivative of it and equate it to zero. That’s what we are going to do here.\nQuestion: You might ask, the derivative of a function is zero at it’s maximum point as well. Why couldn’t that be the case?\nAnswer: Well, the thing is when you take the second derivative of the function (the Hessian matrix), it turns out to be positive semi definite, which proves that loss function is convex and hence the point is a minimum point.\nThe proof of this outside the scope of this blog. For now, just know that the critical point (point at which first derivative is zero) is the minimum of the function.\nContinuing from the above equation, $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ J(\\theta) = \\frac{1}{2} (X\\theta - y)^T (X\\theta - y) $$ Now, $$ \\nabla_θ J(θ) = 0 $$ $$ \\nabla_θ \\left[ \\frac{1}{2} (X\\theta - y)^T (X\\theta - y) \\right] = 0 $$ Expanding, we get: $$ \\frac{1}{2} \\nabla_θ \\left[ (X\\theta)^T X\\theta - (X\\theta)^T y - y^T (X\\theta) + y^T y \\right] = 0 $$\nSince $y^T y$ is independent of $\\theta$, its derivative is zero.\nAlso, by using the property $(ab)^T = b^Ta^T$, we get:\n$$ \\frac{1}{2} \\nabla_θ \\left[ \\theta^TX^TX\\theta - (X\\theta)^T y - y^T (X\\theta) \\right] = 0 $$\nNow, by using the property, if $a^Tb$ is a scalar $a^Tb = b^Ta$, we get: $$ \\frac{1}{2} \\nabla_θ \\left[ \\theta^TX^TX\\theta - y^T (X\\theta) - y^T (X\\theta) \\right] = 0 $$ $$ \\frac{1}{2} \\nabla_θ \\left[ \\theta^TX^TX\\theta - 2y^T (X\\theta) \\right] = 0 $$ $$ \\frac{1}{2} \\nabla_θ \\left[ \\theta^TX^TX\\theta - 2(X^Ty)^T\\theta \\right] = 0 $$\nNow, let us take a step back and look into vector and matrix differentiation.\nScalar First, Then Shape-Check Matrix calculus can look overwhelming, but one of the most effective ways to simplify derivations is to break the process into two manageable stages:\nScalar Differentiation: In this phase, you “pretend” that all the involved expressions are scalar functions—even if they originally represent vectors or matrices. Then, you can apply familiar differentiation rules (such as the power rule, product rule, and chain rule).\nRearrange and check shapes: Once you have a derivative expression based on the scalar reasoning, you will have to rearrange the terms so that the shapes are compatible and the final shape matches the shape of the expression you are differentiating with.\nContinuing the above derivation,\n$$ \\frac{1}{2} \\nabla_θ \\left[ \\theta^TX^TX\\theta - 2(X^Ty)^T\\theta \\right] = 0 $$\nUsing the rule $\\frac{d}{d\\theta}(a * \\theta^2) = 2a \\theta$,\n$$ \\frac{1}{2} \\left[2XX^T\\theta - 2(X^Ty) \\right] = 0 $$\n$$ XX^T\\theta - X^y = 0 $$\n$$ \\theta = (X^TX)^{-1}X^Ty $$\nProblems with the Normal equation Computational cost in high dimensions.\nIn the calculation of normal equation, you will have the calculate $(X^TX)^{-1}$. Taking the inverse1 of a matrix is a $O(d^3)$ operation which is very slow if the number of features of the input matrix is very large. Non invertability\nIn some cases, the matrix is not invertible (for example, when the features are highly collinear) and in such cases it becomes very hard to find the solution. Practical Memory Constraints\nFor very large data sets, storing and manipulating the dense $d\\times d$ matrix $X^TX$ may exceed available memory, even if an inverse could in principle be computed. Introducing Gradient Descent Footnotes Time complexity of Matrix Inversion ↩︎\n",
  "wordCount" : "1338",
  "inLanguage": "en",
  "datePublished": "2025-02-28T19:49:18+05:30",
  "dateModified": "2025-02-28T19:49:18+05:30",
  "author":[{
    "@type": "Person",
    "name": "Vamsi"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/docs/linear_regression/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Vamsi's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Vamsi&#39;s Blog (Alt + H)">Vamsi&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Linear Regression From Scratch
    </h1>
    <div class="post-description">
      In depth explanation of one of the most fundamental Machine Learning algorithm.
    </div>
    <div class="post-meta"><span title='2025-02-28 19:49:18 +0530 IST'>February 28, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Vamsi&nbsp;|&nbsp;<a href="https://github.com/sunny7712/blog/tree/main/content/docs/linear_regression/index.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#loss-function" aria-label="Loss Function">Loss Function</a></li>
                <li>
                    <a href="#solution-for-the-equation" aria-label="Solution for the Equation">Solution for the Equation</a></li>
                <li>
                    <a href="#scalar-first-then-shape-check" aria-label="Scalar First, Then Shape-Check">Scalar First, Then Shape-Check</a></li>
                <li>
                    <a href="#problems-with-the-normal-equation" aria-label="Problems with the Normal equation">Problems with the Normal equation</a></li>
                <li>
                    <a href="#introducing-gradient-descent" aria-label="Introducing Gradient Descent">Introducing Gradient Descent</a></li>
                <li>
                    <a href="#footnotes" aria-label="Footnotes">Footnotes</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.</p>
<p>So in this blog, I have four main goals:</p>
<ul>
<li>
<p>Derive the <strong>normal equation</strong> of LR in batch form. Along the way, we’ll go through a bit on how we tackle differentiation of vectors and matrices in the context of Machine Learning.</p>
</li>
<li>
<p><strong>Justify the Least Squares Error</strong> as the loss function for Linear Regression — why it makes sense mathematically and intuitively.</p>
</li>
<li>
<p><strong>Explain the need for gradient descent</strong> — why we use it and how it works.</p>
</li>
<li>
<p><strong>Implement everything</strong> we learn using python and numpy.</p>
</li>
</ul>
<hr>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.</p>
<figure>
    <img loading="lazy" src="california_housing_dataset.png"
         alt="California Housing Dataset"/> <figcaption>
            <p><p style='text-align:center;'>California Housing Dataset</p></p>
        </figcaption>
</figure>

<p>Let&rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.</p>
<p>The output is a <strong>scalar value</strong>, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.</p>
<p>Thus,</p>
<ul>
<li>The shape of $X$ is $(n,d)$.</li>
<li>The shape of $y$ is $(n,1)$.</li>
</ul>
<p>Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.</p>
<p>For a single example $x$ which is one single row of $X$,</p>
<p>$$
h_{\theta}(x^{(i)}) = \theta_{1} x_1^{(i)} + \theta_{2} x_2^{(i)} + \theta_{3} x_3^{(i)} + \dots + \theta_{d} x_d^{(i)}
$$</p>
<p>where:</p>
<ul>
<li>$h_θ(x^{(i)})$ is the price of the house for the ith row of X.</li>
<li>$x_1^{(i)},x_2^{(i)},x_3^{(i)} \dots x_d^{(i)}$ are the <strong>features of the house</strong> (e.g., number of bedrooms, house age).</li>
<li>$θ_1,θ_2,θ_3 \dots θ_d$​ are the <strong>weights</strong> (parameters) that determine how much each feature contributes to the price.</li>
</ul>
<p>The above equation can also be written as,</p>
<p>$$
h_{\theta}(x^{(i)}) = x^{(i)} {\theta}
$$</p>
<ul>
<li>
<p>$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)</p>
</li>
<li>
<p>$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.</p>
</li>
</ul>
<p>Now, we can stack all the examples (or rows) together and write it in a compact matrix form like,</p>
<p>$$
\hat{y} = h_{\theta}(X) =  X \theta
$$</p>
<ul>
<li>
<p>$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.</p>
</li>
<li>
<p>$h_θ(X)$ and $\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.</p>
</li>
</ul>
<hr>
<h2 id="loss-function">Loss Function<a hidden class="anchor" aria-hidden="true" href="#loss-function">#</a></h2>
<p>From the above discussion, you must&rsquo;ve figured out that the goal is to find the right value of $\theta$. That is, the $\theta$ which gives the correct value or <strong>value as close as possible to</strong> $y$, given $X$.</p>
<p>Therefore, you can  say that if you&rsquo;re predicted vector $\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to  $\theta$.</p>
<p>So, the goal is to minimize the difference between $y$ and $\hat{y}$.</p>
<p>$$
\theta^{*} = \underset{\theta}{argmin} \sum_{i=1}^n |\hat{y}^{(i)} - y^{(i)}|
$$</p>
<p>where:</p>
<ul>
<li>$\theta^{*}$ is the optimal(right) value of $\theta$.</li>
<li>$\hat{y} = X\theta$ represents the predicted values</li>
<li>$y$ is the actual output vector</li>
<li>The term $|\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point.</li>
<li>Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out.</li>
</ul>
<p>But, in practice, we DO NOT use the above equation. We use,</p>
<p>$$
\theta^{*} = \underset{\theta}{argmin} \sum_{i=1}^n (\hat{y}^{(i)} - y^{(i)})^2
$$</p>
<p>This is because:</p>
<ul>
<li><strong>Mathematical convenience</strong>: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it&rsquo;s minimum (at zero).</li>
</ul>
<figure>
    <img loading="lazy" src="squared_vs_modulus.png"
         alt=" Square function vs modulus function"/> <figcaption>
            <p><p style='text-align:center;'>Square function vs modulus function</p></p>
        </figcaption>
</figure>

<p>Now, in Machine Learning, a <strong>Loss Function</strong> measures how well our model&rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\hat{y}$ and the actual output $y$.</p>
<p>Let&rsquo;s represent Loss function with $J(\theta)$. That implies,</p>
<p>$$
J(\theta) = \sum_{i=1}^n (\hat{y}^{(i)} - y^{(i)})^2
$$
$$
J(\theta) = ||\hat{y} - y||_2
$$
$$
\theta^{*} = \underset{\theta}{argmin} J(\theta)
$$</p>
<p>where $||\hat{y} - y||_2$ is the $L2$ norm (vector representation).</p>
<p>The above loss function is also called the <strong>Least Squared Error</strong>.</p>
<blockquote>
<p><em>Note</em>: Here, we derived the loss function of Linear Regression intuitively. There&rsquo;s also a more rigorous mathematical derivation which we will discuss later in the blog.</p></blockquote>
<hr>
<h2 id="solution-for-the-equation">Solution for the Equation<a hidden class="anchor" aria-hidden="true" href="#solution-for-the-equation">#</a></h2>
<p>If you can recall, to find the minimum of any function, we take the derivative of it and equate it to zero. That&rsquo;s what we are going to do here.</p>
<blockquote>
<p><strong>Question</strong>: You might ask, the derivative of a function is zero at it&rsquo;s maximum point as well. Why couldn&rsquo;t that be the case?</p>
<p><strong>Answer</strong>: Well, the thing is when you take the second derivative of the function (the Hessian matrix), it turns out to be positive semi definite, which proves that loss function is convex and hence the point is a minimum point.</p>
<p>The proof of this outside the scope of this blog. For now, just know that the critical point (point at which first derivative is zero) is the minimum of the function.</p></blockquote>
<p>Continuing from the above equation,
$$
J(\theta) = ||\hat{y} - y||_2
$$
$$
J(\theta) = \frac{1}{2} (X\theta - y)^T (X\theta - y)
$$
Now,
$$
\nabla_θ J(θ) = 0
$$
$$
\nabla_θ \left[ \frac{1}{2} (X\theta - y)^T (X\theta - y) \right] = 0
$$
Expanding, we get:
$$
\frac{1}{2} \nabla_θ \left[ (X\theta)^T X\theta - (X\theta)^T y - y^T (X\theta) + y^T y \right] = 0
$$</p>
<p>Since $y^T y$ is independent of $\theta$, its derivative is zero.</p>
<p>Also, by using the property $(ab)^T = b^Ta^T$, we get:</p>
<p>$$
\frac{1}{2} \nabla_θ \left[ \theta^TX^TX\theta - (X\theta)^T y - y^T (X\theta) \right] = 0
$$</p>
<p>Now, by using the property, if $a^Tb$ is a scalar $a^Tb = b^Ta$, we get:
$$
\frac{1}{2} \nabla_θ \left[ \theta^TX^TX\theta - y^T (X\theta) - y^T (X\theta) \right] = 0
$$
$$
\frac{1}{2} \nabla_θ \left[ \theta^TX^TX\theta - 2y^T (X\theta) \right] = 0
$$
$$
\frac{1}{2} \nabla_θ \left[ \theta^TX^TX\theta - 2(X^Ty)^T\theta \right] = 0
$$</p>
<p>Now, let us take a step back and look into vector and matrix differentiation.</p>
<hr>
<h2 id="scalar-first-then-shape-check">Scalar First, Then Shape-Check<a hidden class="anchor" aria-hidden="true" href="#scalar-first-then-shape-check">#</a></h2>
<p>Matrix calculus can look overwhelming, but one of the most effective ways to simplify derivations is to break the process into two manageable stages:</p>
<ol>
<li>
<p><strong>Scalar Differentiation</strong>: In this phase, you “pretend” that all the involved expressions are scalar functions—even if they originally represent vectors or matrices. Then, you can apply familiar differentiation rules (such as the power rule, product rule, and chain rule).</p>
</li>
<li>
<p><strong>Rearrange and check shapes</strong>: Once you have a derivative expression based on the scalar reasoning, you will have to rearrange the terms so that the shapes are compatible and the final shape matches the shape of the expression you are differentiating with.</p>
</li>
</ol>
<p>Continuing the above derivation,</p>
<p>$$
\frac{1}{2} \nabla_θ \left[ \theta^TX^TX\theta - 2(X^Ty)^T\theta \right] = 0
$$</p>
<p>Using the rule $\frac{d}{d\theta}(a * \theta^2) = 2a \theta$,</p>
<p>$$
\frac{1}{2} \left[2XX^T\theta - 2(X^Ty) \right] = 0
$$</p>
<p>$$
XX^T\theta - X^y = 0
$$</p>
<p>$$
\theta = (X^TX)^{-1}X^Ty
$$</p>
<hr>
<h2 id="problems-with-the-normal-equation">Problems with the Normal equation<a hidden class="anchor" aria-hidden="true" href="#problems-with-the-normal-equation">#</a></h2>
<ol>
<li>
<p><strong>Computational cost in high dimensions.</strong></p>
<ul>
<li>In the calculation of normal equation, you will have the calculate $(X^TX)^{-1}$. Taking the inverse<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> of a matrix is a $O(d^3)$ operation which is very slow if the number of features of the input matrix is very large.</li>
</ul>
</li>
<li>
<p><strong>Non invertability</strong></p>
<ul>
<li>In some cases, the matrix is not invertible (for example, when the features are highly collinear) and in such cases it becomes very hard to find the solution.</li>
</ul>
</li>
<li>
<p><strong>Practical Memory Constraints</strong></p>
<ul>
<li>For very large data sets, storing and manipulating the dense $d\times d$ matrix $X^TX$ may exceed available memory, even if an inverse could in principle be computed.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="introducing-gradient-descent">Introducing Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#introducing-gradient-descent">#</a></h2>
<h2 id="footnotes">Footnotes<a hidden class="anchor" aria-hidden="true" href="#footnotes">#</a></h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra">Time complexity of Matrix Inversion</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression From Scratch on x"
            href="https://x.com/intent/tweet/?text=Linear%20Regression%20From%20Scratch&amp;url=http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression From Scratch on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f&amp;title=Linear%20Regression%20From%20Scratch&amp;summary=Linear%20Regression%20From%20Scratch&amp;source=http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression From Scratch on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f&title=Linear%20Regression%20From%20Scratch">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression From Scratch on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression From Scratch on whatsapp"
            href="https://api.whatsapp.com/send?text=Linear%20Regression%20From%20Scratch%20-%20http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression From Scratch on telegram"
            href="https://telegram.me/share/url?text=Linear%20Regression%20From%20Scratch&amp;url=http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Linear Regression From Scratch on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Linear%20Regression%20From%20Scratch&u=http%3a%2f%2flocalhost%3a1313%2fdocs%2flinear_regression%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© Vamsi</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
